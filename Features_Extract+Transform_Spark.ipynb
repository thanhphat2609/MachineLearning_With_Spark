{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyPsppS2ta0y2qnga9HtyMLD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **1. Install and import necessary libraries** #"],"metadata":{"id":"cVKLt5OV0gwf"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q48MSVSW0crZ","executionInfo":{"status":"ok","timestamp":1708001578492,"user_tz":-420,"elapsed":58285,"user":{"displayName":"Phát Nguyễn Thành","userId":"11387313374912510012"}},"outputId":"0c04e026-dd7e-4fd3-f6cc-29f06b331bc7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyspark\n","  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n","Building wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425345 sha256=9c98d4af5bed340af257479977d85c4c560c5a242f6b0876f28a13f919633907\n","  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n","Successfully built pyspark\n","Installing collected packages: pyspark\n","Successfully installed pyspark-3.5.0\n"]}],"source":["# Install pyspark\n","!pip install pyspark"]},{"cell_type":"code","source":["# Import Spark Session\n","from pyspark.sql import SparkSession\n","\n","# Import rand\n","from pyspark.sql.functions import rand"],"metadata":{"id":"ITpGcjnn0rYs","executionInfo":{"status":"ok","timestamp":1708001581711,"user_tz":-420,"elapsed":3,"user":{"displayName":"Phát Nguyễn Thành","userId":"11387313374912510012"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# Create Spark Session\n","spark = SparkSession.builder.appName(\"Features_Extract+Transform_Spark\").getOrCreate()\n","spark.version"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"nVB9pVPN09ff","executionInfo":{"status":"ok","timestamp":1708001627463,"user_tz":-420,"elapsed":7787,"user":{"displayName":"Phát Nguyễn Thành","userId":"11387313374912510012"}},"outputId":"3c130b88-5794-4e9d-e2db-fef7c6cd8e22"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'3.5.0'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","source":["# **2. Tokenizer** #"],"metadata":{"id":"y2596Las1IPb"}},{"cell_type":"code","source":["# Import tokenizer\n","from pyspark.ml.feature import Tokenizer"],"metadata":{"id":"gwTQ1hPT1GtT","executionInfo":{"status":"ok","timestamp":1708002446061,"user_tz":-420,"elapsed":382,"user":{"displayName":"Phát Nguyễn Thành","userId":"11387313374912510012"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# Create a sample dataframe\n","sentenceDataFrame = spark.createDataFrame([\n","    (1, \"Spark is a distributed computing system.\"),\n","    (2, \"It provides interfaces for multiple languages\"),\n","    (3, \"Spark is built on top of Hadoop\")\n","], [\"id\", \"sentence\"])"],"metadata":{"id":"DXiixBgE4MRK","executionInfo":{"status":"ok","timestamp":1708002460678,"user_tz":-420,"elapsed":6193,"user":{"displayName":"Phát Nguyễn Thành","userId":"11387313374912510012"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# Show few rows of dataframe\n","sentenceDataFrame.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hExdngAA4Sh5","executionInfo":{"status":"ok","timestamp":1708002479561,"user_tz":-420,"elapsed":6794,"user":{"displayName":"Phát Nguyễn Thành","userId":"11387313374912510012"}},"outputId":"07bdfd75-a6fe-4179-e95c-2892c9f72008"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["+---+--------------------+\n","| id|            sentence|\n","+---+--------------------+\n","|  1|Spark is a distri...|\n","|  2|It provides inter...|\n","|  3|Spark is built on...|\n","+---+--------------------+\n","\n"]}]},{"cell_type":"code","source":["# Create tokenizer instance.\n","# Mention the column to be tokenized as inputcol\n","# Mention the output column name where the tokens are to be stored.\n","tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")"],"metadata":{"id":"C1OxP3Iw4W4Z","executionInfo":{"status":"ok","timestamp":1708002549753,"user_tz":-420,"elapsed":8,"user":{"displayName":"Phát Nguyễn Thành","userId":"11387313374912510012"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# Tokenize\n","token_df = tokenizer.transform(sentenceDataFrame)"],"metadata":{"id":"ypNYDPwn4m2C","executionInfo":{"status":"ok","timestamp":1708002551586,"user_tz":-420,"elapsed":391,"user":{"displayName":"Phát Nguyễn Thành","userId":"11387313374912510012"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# Display the tokenized data\n","token_df.show(truncate = False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q5yeLhm64nt7","executionInfo":{"status":"ok","timestamp":1708002554523,"user_tz":-420,"elapsed":1821,"user":{"displayName":"Phát Nguyễn Thành","userId":"11387313374912510012"}},"outputId":"042ded11-4af4-4d3d-95c6-cbe610bfc7c6"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["+---+---------------------------------------------+----------------------------------------------------+\n","|id |sentence                                     |words                                               |\n","+---+---------------------------------------------+----------------------------------------------------+\n","|1  |Spark is a distributed computing system.     |[spark, is, a, distributed, computing, system.]     |\n","|2  |It provides interfaces for multiple languages|[it, provides, interfaces, for, multiple, languages]|\n","|3  |Spark is built on top of Hadoop              |[spark, is, built, on, top, of, hadoop]             |\n","+---+---------------------------------------------+----------------------------------------------------+\n","\n"]}]},{"cell_type":"markdown","source":["# **3. Count Vectorizer** #"],"metadata":{"id":"Pt3hZT6w5HiB"}},{"cell_type":"code","source":["# Import CountVectorizer\n","from pyspark.ml.feature import CountVectorizer"],"metadata":{"id":"6BHIzimO4qgb","executionInfo":{"status":"ok","timestamp":1708002795958,"user_tz":-420,"elapsed":373,"user":{"displayName":"Phát Nguyễn Thành","userId":"11387313374912510012"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# Create a sample dataframe and display it.\n","textdata = [(1, \"I love Spark Spark provides Python API \".split()),\n","            (2, \"I love Python Spark supports Python\".split()),\n","            (3, \"Spark solves the big problem of big data\".split())]\n","\n","textdata = spark.createDataFrame(textdata, [\"id\", \"words\"])\n","\n","textdata.show(truncate=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6AqKLb1o5l4V","executionInfo":{"status":"ok","timestamp":1708002803493,"user_tz":-420,"elapsed":1246,"user":{"displayName":"Phát Nguyễn Thành","userId":"11387313374912510012"}},"outputId":"b88c0fe9-08a5-448b-b4e5-dfe08d2038c3"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["+---+-------------------------------------------------+\n","|id |words                                            |\n","+---+-------------------------------------------------+\n","|1  |[I, love, Spark, Spark, provides, Python, API]   |\n","|2  |[I, love, Python, Spark, supports, Python]       |\n","|3  |[Spark, solves, the, big, problem, of, big, data]|\n","+---+-------------------------------------------------+\n","\n"]}]},{"cell_type":"code","source":["# Create a CountVectorizer object\n","# Mention the column to be count vectorized as inputcol\n","# Mention the output column name where the count vectors are to be stored.\n","cv = CountVectorizer(inputCol=\"words\", outputCol=\"features\")"],"metadata":{"id":"bJ1_b0R35nfi","executionInfo":{"status":"ok","timestamp":1708002842031,"user_tz":-420,"elapsed":536,"user":{"displayName":"Phát Nguyễn Thành","userId":"11387313374912510012"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["# Fit the CountVectorizer model on the input data\n","model = cv.fit(textdata)"],"metadata":{"id":"Nh-yD4b95uwx","executionInfo":{"status":"ok","timestamp":1708002844195,"user_tz":-420,"elapsed":1592,"user":{"displayName":"Phát Nguyễn Thành","userId":"11387313374912510012"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["# Transform the input data to bag-of-words vectors\n","result = model.transform(textdata)"],"metadata":{"id":"tkj92fNz5vmy","executionInfo":{"status":"ok","timestamp":1708002844596,"user_tz":-420,"elapsed":403,"user":{"displayName":"Phát Nguyễn Thành","userId":"11387313374912510012"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["# display the dataframe\n","result.show(truncate=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vt_YDtmI5wnx","executionInfo":{"status":"ok","timestamp":1708002845926,"user_tz":-420,"elapsed":1332,"user":{"displayName":"Phát Nguyễn Thành","userId":"11387313374912510012"}},"outputId":"ef96872e-a26e-4a8f-e5b5-5fed2df5ed52"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["+---+-------------------------------------------------+----------------------------------------------------+\n","|id |words                                            |features                                            |\n","+---+-------------------------------------------------+----------------------------------------------------+\n","|1  |[I, love, Spark, Spark, provides, Python, API]   |(13,[0,1,2,3,6,8],[2.0,1.0,1.0,1.0,1.0,1.0])        |\n","|2  |[I, love, Python, Spark, supports, Python]       |(13,[0,1,2,3,12],[1.0,2.0,1.0,1.0,1.0])             |\n","|3  |[Spark, solves, the, big, problem, of, big, data]|(13,[0,4,5,7,9,10,11],[1.0,2.0,1.0,1.0,1.0,1.0,1.0])|\n","+---+-------------------------------------------------+----------------------------------------------------+\n","\n"]}]},{"cell_type":"markdown","source":["# **4. TF-IDF** #"],"metadata":{"id":"Niv_O_YJ5zcC"}},{"cell_type":"code","source":["# Import necessary classes for TF-IDF calculation\n","from pyspark.ml.feature import HashingTF, IDF, Tokenizer"],"metadata":{"id":"FpxAMXqr5xdO","executionInfo":{"status":"ok","timestamp":1708002948133,"user_tz":-420,"elapsed":2,"user":{"displayName":"Phát Nguyễn Thành","userId":"11387313374912510012"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["# Create a sample dataframe and display it.\n","sentenceData = spark.createDataFrame([\n","        (1, \"Spark supports python\"),\n","        (2, \"Spark is fast\"),\n","        (3, \"Spark is easy\")\n","    ], [\"id\", \"sentence\"])\n","\n","sentenceData.show(truncate = False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"laS64Fgq6LE6","executionInfo":{"status":"ok","timestamp":1708002963100,"user_tz":-420,"elapsed":937,"user":{"displayName":"Phát Nguyễn Thành","userId":"11387313374912510012"}},"outputId":"2a16473d-28e7-4f26-dbe0-cac4973e6a2a"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["+---+---------------------+\n","|id |sentence             |\n","+---+---------------------+\n","|1  |Spark supports python|\n","|2  |Spark is fast        |\n","|3  |Spark is easy        |\n","+---+---------------------+\n","\n"]}]},{"cell_type":"code","source":["# Tokenize the \"sentence\" column and store in the column \"words\"\n","tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n","wordsData = tokenizer.transform(sentenceData)\n","wordsData.show(truncate = False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EN-hE54-6Nib","executionInfo":{"status":"ok","timestamp":1708002964029,"user_tz":-420,"elapsed":931,"user":{"displayName":"Phát Nguyễn Thành","userId":"11387313374912510012"}},"outputId":"2bdff509-08d6-498f-d7fe-09d3da798f6f"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["+---+---------------------+-------------------------+\n","|id |sentence             |words                    |\n","+---+---------------------+-------------------------+\n","|1  |Spark supports python|[spark, supports, python]|\n","|2  |Spark is fast        |[spark, is, fast]        |\n","|3  |Spark is easy        |[spark, is, easy]        |\n","+---+---------------------+-------------------------+\n","\n"]}]},{"cell_type":"code","source":["# Create a HashingTF object\n","# mention the \"words\" column as input\n","# mention the \"rawFeatures\" column as output\n","\n","hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=10)\n","featurizedData = hashingTF.transform(wordsData)\n","\n","featurizedData.show(truncate = False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XXOVqDtP6PIM","executionInfo":{"status":"ok","timestamp":1708002987307,"user_tz":-420,"elapsed":1305,"user":{"displayName":"Phát Nguyễn Thành","userId":"11387313374912510012"}},"outputId":"933bc90c-35ab-4ec8-f34d-1acd7e6bdcec"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["+---+---------------------+-------------------------+--------------------------+\n","|id |sentence             |words                    |rawFeatures               |\n","+---+---------------------+-------------------------+--------------------------+\n","|1  |Spark supports python|[spark, supports, python]|(10,[4,6,9],[1.0,1.0,1.0])|\n","|2  |Spark is fast        |[spark, is, fast]        |(10,[3,6,9],[1.0,1.0,1.0])|\n","|3  |Spark is easy        |[spark, is, easy]        |(10,[0,6,9],[1.0,1.0,1.0])|\n","+---+---------------------+-------------------------+--------------------------+\n","\n"]}]},{"cell_type":"code","source":["# Create an IDF object\n","# mention the \"rawFeatures\" column as input\n","# mention the \"features\" column as output\n","\n","idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n","idfModel = idf.fit(featurizedData)\n","tfidfData = idfModel.transform(featurizedData)"],"metadata":{"id":"Id2v5Byh6RSn","executionInfo":{"status":"ok","timestamp":1708002992266,"user_tz":-420,"elapsed":4963,"user":{"displayName":"Phát Nguyễn Thành","userId":"11387313374912510012"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["# Display the tf-idf data\n","tfidfData.select(\"sentence\", \"features\").show(truncate=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LxPOGOQO6SOo","executionInfo":{"status":"ok","timestamp":1708002993388,"user_tz":-420,"elapsed":1141,"user":{"displayName":"Phát Nguyễn Thành","userId":"11387313374912510012"}},"outputId":"35f85e36-3c73-4544-bd1d-d8c8d8d148e3"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["+---------------------+-----------------------------------------+\n","|sentence             |features                                 |\n","+---------------------+-----------------------------------------+\n","|Spark supports python|(10,[4,6,9],[0.6931471805599453,0.0,0.0])|\n","|Spark is fast        |(10,[3,6,9],[0.6931471805599453,0.0,0.0])|\n","|Spark is easy        |(10,[0,6,9],[0.6931471805599453,0.0,0.0])|\n","+---------------------+-----------------------------------------+\n","\n"]}]},{"cell_type":"markdown","source":["# **5. StopWordsRemover** #"],"metadata":{"id":"IOs9b8pu6Wpn"}},{"cell_type":"code","source":["# Import StopWordsRemover\n","from pyspark.ml.feature import StopWordsRemover"],"metadata":{"id":"BZ0WcN3M6ad0","executionInfo":{"status":"ok","timestamp":1708003018004,"user_tz":-420,"elapsed":2,"user":{"displayName":"Phát Nguyễn Thành","userId":"11387313374912510012"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["# Create a dataframe with sample text and display it\n","textData = spark.createDataFrame([\n","    (1, ['Spark', 'is', 'an', 'open-source', 'distributed', 'computing', 'system']),\n","    (2, ['IT', 'has', 'interfaces', 'for', 'multiple', 'languages']),\n","    (3, ['It', 'has', 'a', 'wide', 'range', 'of', 'libraries', 'and', 'APIs'])\n","], [\"id\", \"sentence\"])\n","\n","textData.show(truncate = False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ciOgX_Jo6cG7","executionInfo":{"status":"ok","timestamp":1708003025315,"user_tz":-420,"elapsed":881,"user":{"displayName":"Phát Nguyễn Thành","userId":"11387313374912510012"}},"outputId":"c1a73241-fb25-4850-9faa-043a5a922385"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["+---+------------------------------------------------------------+\n","|id |sentence                                                    |\n","+---+------------------------------------------------------------+\n","|1  |[Spark, is, an, open-source, distributed, computing, system]|\n","|2  |[IT, has, interfaces, for, multiple, languages]             |\n","|3  |[It, has, a, wide, range, of, libraries, and, APIs]         |\n","+---+------------------------------------------------------------+\n","\n"]}]},{"cell_type":"code","source":["# Remove stopwords from \"sentence\" column and store the result in \"filtered_sentence\" column\n","remover = StopWordsRemover(inputCol=\"sentence\", outputCol=\"filtered_sentence\")\n","textData = remover.transform(textData)"],"metadata":{"id":"wnNTudhj6dtE","executionInfo":{"status":"ok","timestamp":1708003030701,"user_tz":-420,"elapsed":346,"user":{"displayName":"Phát Nguyễn Thành","userId":"11387313374912510012"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["# Display the dataframe\n","textData.show(truncate = False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kcj2r3It6e7k","executionInfo":{"status":"ok","timestamp":1708003037634,"user_tz":-420,"elapsed":776,"user":{"displayName":"Phát Nguyễn Thành","userId":"11387313374912510012"}},"outputId":"42107781-4763-4cf7-e6a6-ead55397a792"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["+---+------------------------------------------------------------+----------------------------------------------------+\n","|id |sentence                                                    |filtered_sentence                                   |\n","+---+------------------------------------------------------------+----------------------------------------------------+\n","|1  |[Spark, is, an, open-source, distributed, computing, system]|[Spark, open-source, distributed, computing, system]|\n","|2  |[IT, has, interfaces, for, multiple, languages]             |[interfaces, multiple, languages]                   |\n","|3  |[It, has, a, wide, range, of, libraries, and, APIs]         |[wide, range, libraries, APIs]                      |\n","+---+------------------------------------------------------------+----------------------------------------------------+\n","\n"]}]},{"cell_type":"markdown","source":["# **6. StringIndexer** #"],"metadata":{"id":"wENN1bJC6iTo"}},{"cell_type":"code","source":["# Import StringIndexer\n","from pyspark.ml.feature import StringIndexer"],"metadata":{"id":"PUvKE4w56gst","executionInfo":{"status":"ok","timestamp":1708003059707,"user_tz":-420,"elapsed":3,"user":{"displayName":"Phát Nguyễn Thành","userId":"11387313374912510012"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["# Create a dataframe with sample text and display it\n","colors = spark.createDataFrame(\n","    [(0, \"red\"), (1, \"red\"), (2, \"blue\"), (3, \"yellow\" ), (4, \"yellow\"), (5, \"yellow\")],\n","    [\"id\", \"color\"])\n","\n","colors.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o6bNgYFZ6mU5","executionInfo":{"status":"ok","timestamp":1708003065328,"user_tz":-420,"elapsed":524,"user":{"displayName":"Phát Nguyễn Thành","userId":"11387313374912510012"}},"outputId":"215d6b49-2ccb-4360-99c6-d5aedf697c80"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["+---+------+\n","| id| color|\n","+---+------+\n","|  0|   red|\n","|  1|   red|\n","|  2|  blue|\n","|  3|yellow|\n","|  4|yellow|\n","|  5|yellow|\n","+---+------+\n","\n"]}]},{"cell_type":"code","source":["# Index the strings in the column \"color\" and store their indexes in the column \"colorIndex\"\n","indexer = StringIndexer(inputCol=\"color\", outputCol=\"colorIndex\")\n","indexed = indexer.fit(colors).transform(colors)"],"metadata":{"id":"hXqnYNaH6nh_","executionInfo":{"status":"ok","timestamp":1708003072307,"user_tz":-420,"elapsed":2135,"user":{"displayName":"Phát Nguyễn Thành","userId":"11387313374912510012"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["# Display the dataframe\n","indexed.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p381CIJn6o1q","executionInfo":{"status":"ok","timestamp":1708003077536,"user_tz":-420,"elapsed":519,"user":{"displayName":"Phát Nguyễn Thành","userId":"11387313374912510012"}},"outputId":"a39ad695-4899-4efb-aa23-3f27b5040409"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["+---+------+----------+\n","| id| color|colorIndex|\n","+---+------+----------+\n","|  0|   red|       1.0|\n","|  1|   red|       1.0|\n","|  2|  blue|       2.0|\n","|  3|yellow|       0.0|\n","|  4|yellow|       0.0|\n","|  5|yellow|       0.0|\n","+---+------+----------+\n","\n"]}]},{"cell_type":"markdown","source":["# **7. Standard Scaler** #"],"metadata":{"id":"SfuiXgDk6t1R"}},{"cell_type":"code","source":["# Import StandardScaler\n","from pyspark.ml.feature import StandardScaler"],"metadata":{"id":"3kXcsq3i6qN9","executionInfo":{"status":"ok","timestamp":1708003117077,"user_tz":-420,"elapsed":18,"user":{"displayName":"Phát Nguyễn Thành","userId":"11387313374912510012"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["# Create a sample dataframe and display it\n","from pyspark.ml.linalg import Vectors\n","data = [(1, Vectors.dense([70, 170, 17])),\n","        (2, Vectors.dense([80, 165, 25])),\n","        (3, Vectors.dense([65, 150, 135]))]\n","df = spark.createDataFrame(data, [\"id\", \"features\"])\n","\n","df.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fAmCIrsC60Qx","executionInfo":{"status":"ok","timestamp":1708003123636,"user_tz":-420,"elapsed":2374,"user":{"displayName":"Phát Nguyễn Thành","userId":"11387313374912510012"}},"outputId":"7def9e70-e2a6-480e-ac55-71629c3a5bf0"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["+---+------------------+\n","| id|          features|\n","+---+------------------+\n","|  1| [70.0,170.0,17.0]|\n","|  2| [80.0,165.0,25.0]|\n","|  3|[65.0,150.0,135.0]|\n","+---+------------------+\n","\n"]}]},{"cell_type":"code","source":["# Define the StandardScaler transformer\n","scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True, withMean=True)"],"metadata":{"id":"ksbC7vm461PS","executionInfo":{"status":"ok","timestamp":1708003126894,"user_tz":-420,"elapsed":4,"user":{"displayName":"Phát Nguyễn Thành","userId":"11387313374912510012"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["# Fit the transformer to the dataset\n","scalerModel = scaler.fit(df)"],"metadata":{"id":"NwRtWj5u62kX","executionInfo":{"status":"ok","timestamp":1708003131712,"user_tz":-420,"elapsed":1133,"user":{"displayName":"Phát Nguyễn Thành","userId":"11387313374912510012"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["# Scale the data\n","scaledData = scalerModel.transform(df)"],"metadata":{"id":"PtchRr8D63ej","executionInfo":{"status":"ok","timestamp":1708003133511,"user_tz":-420,"elapsed":524,"user":{"displayName":"Phát Nguyễn Thành","userId":"11387313374912510012"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["# Show the scaled data\n","scaledData.show(truncate = False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iBG2CJQl64L5","executionInfo":{"status":"ok","timestamp":1708003141084,"user_tz":-420,"elapsed":601,"user":{"displayName":"Phát Nguyễn Thành","userId":"11387313374912510012"}},"outputId":"8a95c481-e1b0-4440-8bb9-1f0ddd9e5ba3"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["+---+------------------+------------------------------------------------------------+\n","|id |features          |scaledFeatures                                              |\n","+---+------------------+------------------------------------------------------------+\n","|1  |[70.0,170.0,17.0] |[-0.218217890235993,0.8006407690254366,-0.6369487984517485] |\n","|2  |[80.0,165.0,25.0] |[1.0910894511799611,0.32025630761017515,-0.5156252177942725]|\n","|3  |[65.0,150.0,135.0]|[-0.8728715609439701,-1.120897076635609,1.152574016246021]  |\n","+---+------------------+------------------------------------------------------------+\n","\n"]}]},{"cell_type":"markdown","source":["# **8. Stop Spark Session** #"],"metadata":{"id":"bvv8TtvR66WI"}},{"cell_type":"code","source":["spark.stop()"],"metadata":{"id":"8LREAmAm651U","executionInfo":{"status":"ok","timestamp":1708003155167,"user_tz":-420,"elapsed":506,"user":{"displayName":"Phát Nguyễn Thành","userId":"11387313374912510012"}}},"execution_count":38,"outputs":[]}]}